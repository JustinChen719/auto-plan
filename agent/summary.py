from agent.base import BaseAgent
from core.llm import LLMName, LLM, LLMResponseContent
from tools import get_tool_mapper, CreateFileTool
from utils import get_logger

logger = get_logger(__name__)


class SummaryAgent(BaseAgent):
    name: str = "SummaryAgent"
    description: str = "æ€»ç»“è€…ï¼Œæ ¹æ®ç›®æ ‡å’Œå½“å‰æ‰§è¡ŒçŠ¶æ€ã€æ‰§è¡Œç»“æœï¼Œç»™å‡ºæ€»ç»“ä½œä¸ºæœ€ç»ˆäº¤ç»™ç”¨æˆ·çš„ç­”å¤ã€‚"

    def __init__(self):
        super().__init__()

        self.llm = LLM(LLMName.QWEN_TURBO)

    def reset(self) -> None:
        self.current_query = None
        self.current_results = None
        self.current_step = 0
        self.memory.clear()
        self.finished = False
        self.failed = False
        if self.system_prompt:
            self.memory.add_system_message(self.system_prompt)

    def create_next_step_prompt(self) -> None:
        if self.current_results is not None:
            results = "\n".join(self.current_results)
        else:
            results = ""
        prompt: str = self.next_step_prompt.format(
                query=self.current_query,
                results=results
        )
        self.current_prompt = prompt

    async def think(self) -> LLMResponseContent:
        self.memory.add_user_message(content=self.current_prompt)

        # è·å–å›å¤å†…å®¹
        response: LLMResponseContent = await self.llm.response(
                memory=self.memory,
                tools=[CreateFileTool.get_tool_call_params()],
                tool_choice={"type": "function", "function": {"name": "CreateFileTool"}},  # å¼ºåˆ¶ä½¿ç”¨
                enable_thinking=False,
                display_reasoning_content=False,
                display_content=False,
        )
        self.memory.add_assistant_message(content=response.content)
        return response

    async def step(self) -> LLMResponseContent:
        response = await self.think()
        return response

    async def run(self, query: str, results: list[str]) -> LLMResponseContent:
        logger.info(f"ğŸ“ {self.name} æ­£åœ¨æ€»ç»“ç»“æœ...")

        self.current_query = query
        self.current_results = results

        # åªæ‰§è¡Œä¸€æ­¥
        self.create_next_step_prompt()
        response: LLMResponseContent = await self.step()

        # ä¿å­˜æ–‡ä»¶
        if len(response.tool_calls) == 0:
            logger.info(f"ğŸŸ¡ {self.name} æœªè°ƒç”¨å·¥å…· {"CreateFileTool"}")
        else:
            tool_mapper = get_tool_mapper()
            create_file_tool = tool_mapper.get("CreateFileTool")
            for tool_call in response.tool_calls:
                if tool_call.name == "CreateFileTool":
                    await create_file_tool.execute(tool_call.arguments)
        return response
