from agent.base import BaseAgent
from agent.tool_call import ToolCallAgent
from core.llm import LLM, LLMName, LLMResponseContent
from utils.prompt import PLANNER_JUGER_PROMPT
from utils import get_logger

logger = get_logger(__name__)


class PlannerAgent(BaseAgent):
    name: str = "PlannerAgent"
    description: str = "è§„åˆ’å™¨ï¼Œæ ¹æ®é—®é¢˜ï¼Œç»™å‡ºä¸€ä¸ªè§„åˆ’æ–¹æ¡ˆã€‚"

    def __init__(self, agent_mapper: dict[str, ToolCallAgent]):
        super().__init__()
        self.juger_prompt: str = PLANNER_JUGER_PROMPT

        # åŠŸèƒ½æ€§æ™ºèƒ½ä½“æ˜ å°„é›†
        self.agent_mapper: dict[str, ToolCallAgent] = agent_mapper

        self.memory.add_system_message(self.system_prompt)
        self.llm = LLM(LLMName.QWEN3_32B)

    def reset(self) -> None:
        self.current_query = None
        # self.current_step = 0
        self.memory.clear()
        self.finished = False
        self.failed = False
        if self.system_prompt:
            self.memory.add_system_message(self.system_prompt)

    async def think(self) -> LLMResponseContent:
        # è®¾ç½® prompt
        agents = "\n".join([
            f"{agent.name}: {agent.description}" for agent in self.agent_mapper.values()
        ])
        prompt: str = self.next_step_prompt.format(
                query=self.current_query,
                results="",
                agents=agents)
        self.memory.add_user_message(content=prompt)

        # è·å–å›å¤å†…å®¹
        response: LLMResponseContent = await self.llm.response(
                memory=self.memory,
                display_reasoning_content=False,
                display_content=False,
        )
        self.memory.add_assistant_message(content=response.content)
        return response

    async def step(self) -> LLMResponseContent:
        response: LLMResponseContent = await self.think()
        return response

    async def run(self, query) -> LLMResponseContent:
        logger.info(f"ğŸ“œ {self.name} æ­£åœ¨åˆ¶ä½œè§„åˆ’æ–¹æ¡ˆ...")

        self.current_query = query

        # åªæ‰§è¡Œä¸€æ­¥
        response: LLMResponseContent = await self.step()
        logger.info(f"ğŸ’¡ {self.name} å®Œæˆäº†è®¡åˆ’çš„åˆ¶å®š")
        return response

    async def juger(self, query: str, results: list[str]) -> bool:
        logger.info(f"ğŸ¤” {self.name} æ­£åœ¨åˆ¤æ–­ç»“æœ...")

        # è®¾ç½® prompt
        prompt: str = self.juger_prompt.format(
                query=query,
                results="\n".join(results)
        )
        self.memory.add_user_message(content=prompt)

        # è·å–å›å¤å†…å®¹
        response: LLMResponseContent = await self.llm.response(
                memory=self.memory,
        )
        self.memory.add_assistant_message(content=response.content)

        # å¤„ç†å›å¤å†…å®¹
        can_finish = "False" not in response.content
        if can_finish:
            logger.info(f"ğŸ {self.name} åˆ¤æ–­å®Œæ¯•ï¼Œè¾¾åˆ°ç›®æ ‡è¦æ±‚")
        else:
            logger.info(f"ğŸŸ¡ {self.name} åˆ¤æ–­å®Œæ¯•ï¼Œæœªè¾¾åˆ°ç›®æ ‡è¦æ±‚ï¼Œéœ€è¦å°è¯•ç»§ç»­åˆ¶å®šè®¡åˆ’")
        return can_finish
