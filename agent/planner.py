from agent.base import BaseAgent
from agent.react import ReActAgent
from core.llm import LLM, LLMName, LLMResponseContent
from tools import FinishTool
from utils import get_logger

logger = get_logger(__name__)


class PlannerAgent(BaseAgent):
    name: str = "PlannerAgent"
    description: str = "è§„åˆ’å™¨ï¼Œæ ¹æ®é—®é¢˜ï¼Œç»™å‡ºä¸€ä¸ªè§„åˆ’æ–¹æ¡ˆã€‚"

    def __init__(self, agent_mapper: dict[str, ReActAgent]):
        super().__init__()

        # åŠŸèƒ½æ€§æ™ºèƒ½ä½“æ˜ å°„é›†
        self.agent_mapper: dict[str, ReActAgent] = agent_mapper

        self.llm = LLM(LLMName.QWEN3_235B_A22B)

    def reset(self) -> None:
        self.current_query = None
        self.current_results = None
        self.current_step = 0
        self.memory.clear()
        self.finished = False
        self.failed = False
        if self.system_prompt:
            self.memory.add_system_message(self.system_prompt)

    def add_agent_result(self, call_id: str, result: str) -> None:
        """
        å°†agentè°ƒç”¨ç»“æœï¼Œä»¥tool_callçš„æ–¹å¼ä¿å­˜
        """

        self.memory.add_tool_message(
                content=result,
                tool_call_id=call_id
        )

    def create_next_step_prompt(self) -> None:
        prompt: str = self.next_step_prompt.format(
                query=self.current_query
        )
        self.current_prompt = prompt

    async def think(self) -> LLMResponseContent:
        self.memory.add_user_message(content=self.current_prompt)

        # è®¾ç½® agents
        agents = [agent.get_tool_call_like_params() for agent in self.agent_mapper.values()]
        agents.append(FinishTool.get_tool_call_params())  # æ·»åŠ  finish å·¥å…·

        # è·å–å›å¤å†…å®¹
        response: LLMResponseContent = await self.llm.response(
                memory=self.memory,
                tools=agents,
                display_reasoning_content=False,
                display_content=False
        )
        self.memory.add_assistant_message(content=response.content, tool_calls=response.tool_calls)
        return response

    async def step(self) -> LLMResponseContent:
        response: LLMResponseContent = await self.think()
        return response

    async def run(self, query: str, results: list[str] = None) -> LLMResponseContent:
        logger.info(f"ğŸ¤” {self.name} æ­£åœ¨æ€è€ƒ...")

        self.current_query = query
        self.current_results = results

        # åªæ‰§è¡Œä¸€æ­¥
        self.create_next_step_prompt()
        response: LLMResponseContent = await self.step()
        logger.info(f"ğŸ’¡ {self.name} å®Œæˆ")
        return response
