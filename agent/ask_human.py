from agent.tool_call import ToolCallAgent
from core.llm import LLM, LLMName, LLMResponseContent
from tools import get_tool_mapper, UserInputTool, FinishTool
from utils import get_logger

logger = get_logger(__name__)


class AskHumanAgent(ToolCallAgent):
    name: str = "AskHumanAgent"
    description: str = "åœ¨å¿…è¦æ—¶å‘äººç±»æé—®ï¼Œè¯¢é—®å¾—åˆ°å…³é”®ä¿¡æ¯åç»§ç»­å›ç­”é—®é¢˜ã€‚"

    def __init__(self):
        super().__init__()

        self.allowed_tool_call_names = [
            UserInputTool.name,
            FinishTool.name
        ]

        if self.system_prompt:
            self.memory.add_system_message(self.system_prompt)
        self.llm = LLM(LLMName.QWEN3_32B)

    async def think(self) -> LLMResponseContent:
        # è®¾ç½® prompt
        prompt: str = self.next_step_prompt.format(query=self.current_query)
        self.memory.add_user_message(content=prompt)

        # è®¾ç½® tools
        tool_mapper = get_tool_mapper()
        tools = [tool_mapper[tool_name].get_tool_call_params() for tool_name in self.allowed_tool_call_names]

        # è·å–å›å¤å†…å®¹
        response: LLMResponseContent = await self.llm.response(
                memory=self.memory,
                tools=tools
        )
        self.memory.add_assistant_message(content=response.content, tool_calls=response.tool_calls)
        return response

    async def tool_call(
            self,
            tool_call_id: str,
            tool_call_name: str,
            tool_call_params: dict
    ) -> None | str:
        if tool_call_name not in self.allowed_tool_call_names:
            raise ValueError(f"æ™ºèƒ½ä½“ {self.name} æ²¡æœ‰ {tool_call_name} è¿™ä¸ªå·¥å…·ï¼")

        tool_mapper = get_tool_mapper()
        if tool_call_name in tool_mapper:
            tool = tool_mapper[tool_call_name]
            if tool.check_params(tool_call_params):
                logger.info(f"âš™ï¸ {tool_call_name} å·¥å…·è¢«è°ƒç”¨ï¼")
                # ç»“æŸ
                if tool_call_name == FinishTool.name:
                    self.finished = True
                    self.result = await tool.execute(tool_call_params)
                    return self.result
                # æ­£å¸¸æ‰§è¡Œ
                return await tool.execute(tool_call_params)
            else:
                raise ValueError(f"è°ƒç”¨ {tool_call_name} æ—¶æä¾›çš„å‚æ•° {tool_call_params} æœ‰é—®é¢˜ï¼")
        else:
            raise ValueError(f"æ²¡æœ‰ {tool_call_name} è¿™ä¸ªå·¥å…·ï¼")

    async def step(self) -> LLMResponseContent:
        # æ€è€ƒ
        response: LLMResponseContent = await self.think()

        # æ‰§è¡Œå·¥å…·
        for tool_call in response.tool_calls:
            response_content: str = await self.tool_call(
                    tool_call_id=tool_call.id,
                    tool_call_name=tool_call.name,
                    tool_call_params=tool_call.arguments
            )
            if response_content:
                self.memory.add_tool_message(
                        content=response_content,
                        tool_call_id=tool_call.id
                )

        return response

    async def run(self, query) -> str | None:
        logger.info(f"â”ï¸ {self.name} æ­£åœ¨å‘ç”¨æˆ·æé—®ï¼š{query}")
        self.reset()
        self.current_query = query
        self.current_step = 1
        while not self.finished and not self.failed and self.current_step <= self.max_step:
            await self.step()
            self.current_step += 1

        if self.failed:
            logger.info(f"ğŸ”´ {self.name} è¿è¡Œå‡ºç°é”™è¯¯ï¼")
            return None
        if self.finished:
            logger.info(f"ğŸŸ¢ {self.name} è¿è¡Œå®Œæˆï¼Œç»“æœä¸ºï¼š{self.result}")
            return self.result
